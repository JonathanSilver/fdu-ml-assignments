# k-NN 分类器的实现和应用

## 任务描述

1. 手工实现 k-NN 算法；
2. 使用 k-NN 算法实现对 MNIST 数据集中的手写数字进行分类。

## 数据描述

[MNIST](http://yann.lecun.com/exdb/mnist/)数据集由 60,000 张已标注的训练集和 10,000 张已标注的测试集构成。[数据集使用说明](/1.%20kNN/mnist/README.md)

## 数据预处理

为了比较数据集对算法预测性能的影响，本报告进行 4 组实验，根据数据规模与是否经过预处理两个方面划分实验组。“完整数据” 完整使用 60,000 张训练集图片与 10,000 张测试集图片；“部分数据”随机选取完整训练集中的 6000 条数据作为训练集，随机选取完整测试集中的 1000 条数据作为测试集。未经预处理的数据是“原始”数据；每张图片经过 PCA 降维为 100 维的是“PCA”预处理过的数据。

## 算法实现

本报告主要使用 C++ 实现 kNN 算法，并使用多线程（实验中使用 4 线程）加速。具体来说，假设共有 T 个线程，则将测试集划分为 T 个组，每个组中的图像个数尽可能相近。[实验代码](/1.%20kNN/knn.cpp)

Python 实现的 kNN 算法使用多进程加速， 借助 SharedMemory 在进程中共享数据，尽管如此， Python 版本的 kNN 运行速度依然远远慢于 C++ 版本。 实验中 Python 版本用于处理 PCA 降维后的“完整数据” 与两组“部分数据” 的实验。代码中的二进制输入部分有所参考并在注释中指出，进程间共享数据的方法改编自官方文档。[实验代码](knn.py)

## 实验结果及分析

实验中使用准确率衡量算法的分类效果：

准确率 = 正确分类个数 / 测试集大小

先使用完整、原始数据，通过改变不同的 k（k = 1, 11, 21, ..., 101），得到准确率关于 k 的实验结果：

|k|准确率|
|:-:|:-:|
|1|**0.9691**|
|11|0.9668|
|21|0.9630|
|31|0.9591|
|41|0.9564|
|51|0.9533|
|61|0.9509|
|71|0.9485|
|81|0.9467|
|91|0.9448|
|101|0.9437|

由于准确率随 k 单调递减，所以第二轮实验将 k 的范围定为 1, 3, 5, 7, 9, 11，得到如下实验结果：

|k|完整|数据|部分|数据|
|:-:|:-:|:-:|:-:|:-:|	
||原始|PCA|原始|PCA|
|1|0.9691|0.9719|0.9430|0.9380|
|3|**0.9705**|**0.9744**|0.9430|**0.9460**|
|5|0.9688|0.9737|**0.9440**|0.9400|
|7|0.9694|0.9736|0.9410|0.9380|
|9|0.9659|0.9725|0.9390|0.9380|
|11|0.9668|0.9708|0.9370|0.9310|

除“完整数据” 中“原始” 这组实验算法使用 C++ 实现，其余 3 组实验算法使用 Python 实现。

根据表中所示结果，使用“完整数据”可以得到比使用“部分数据”更高的准确率；且使用 PCA 降维后，可以得到更好的预测效果。

## 总结

本报告手工实现了 kNN 算法， 将其应用于 MNIST 数据集，在未经预处理的原始、完整数据上达到 97.05%的准确率，在经过 PCA 降维的数据上达到 97.44% 的准确率。

kNN 算法的重点在于其运行效率，由于每次预测时，都需要与训练集中所有的图片进行比较，如果训练集较大，那么算法的运行耗时较长，所以在实验中，对于原始、完整数据，使用运行速度较快的 C++ 实现，并使用多线程进一步加速。这里的多线程在使用中无需考虑线程同步问题，因为训练集是只读的，而测试集中的图像被分配到不同的线程中单独计算，互不干扰；类似地，对于经过降维的与规模较小的数据，使用 Python 实现多进程 kNN 算法。
